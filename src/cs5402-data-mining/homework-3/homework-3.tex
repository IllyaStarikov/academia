%
%  exercise-1.tex
%  artificial intelligence
%
%  Created by Illya Starikov on 01/17/18.
%  Copyright 2018. Illya Starikov. All rights reserved.
%

\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[12pt]{scrartcl}

\newcommand{\exercisenumber}{3}
\newcommand{\duedate}{July 17\textsuperscript{th}, 2018}
\input{../macros.tex}
\usepackage{multicol}

\begin{document}
\maketitle

\section{C4.5}
\begin{table}[H]
    \centering
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|c|}\hline
            \textbf{Split} & \textbf{Entropy Less Than} & \textbf{Entropy Greater Than} & \textbf{Entropy Before Split} & \textbf{Entropy After Split} & \textbf{Information Gain}\\\hline
            8              & 0                          & 1.55459                       & 1.579                         & 1.435                        & 0.14400\\
            10             & 1                          & 1.57262                       & 1.579                         & 1.48453                      & 0.09447\\
            15             & 0.9183                     & 1.52193                       & 1.579                         & 1.38263                      & 0.19637\\
            20             & 0.81128                    & 1.39215                       & 1.579                         & 1.21342                      & 0.36558\\
            24             & 0.97095                    & 1.40564                       & 1.579                         & 1.23845                      & 0.34055\\
            28             & 0.9183                     & 0.98523                       & 1.579                         & 0.95434                      & 0.62466\\
            28             & 1.37878                    & 1                             & 1.579                         & 1.20396                      & 0.37504\\
            28             & 1.5                        & 0.97095                       & 1.579                         & 1.29652                      & 0.28248\\
            29             & 1.53049                    & 1                             & 1.579                         & 1.36726                      & 0.21174\\
            30             & 1.52193                    & 0.9183                        & 1.579                         & 1.38263                      & 0.19637\\
            30             & 1.49492                    & 0                             & 1.579                         & 1.26493                      & 0.31407\\
            31             & 1.55459                    & 0                             & 1.579                         & 1.435                        & 0.14400\\
            33             & 1.57662                    & 0                             & 1.579                         & 1.57662                      & 0.00238\\\hline
        \end{tabular}
    }
\end{table}

\section{Grouping}
\begin{table}[H]
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{|l|l|l|ll}
            \hline
            \textbf{Grouping}  & \textbf{Entropy} & \textbf{Entropy Before Split} & \multicolumn{1}{l|}{\textbf{Entropy After Split}} & \multicolumn{1}{l|}{\textbf{Information Gain}} \\ \hline
            Coke-Pepsi         & 1.5219           & 1.565                         & \multicolumn{1}{l|}{1.2652}                       & \multicolumn{1}{l|}{0.2998}                    \\ \hline
            Mountain Dew       & 1.5850           & 1.565                         &                                                   &                                                \\ \hline
            Coke-Mountain Dew  & 1.4591           & 1.565                         & \multicolumn{1}{l|}{1.0943}                       & \multicolumn{1}{l|}{0.4706}                    \\ \hline
            Pepsi              & 0                & 1.565                         &                                                   &                                                \\ \hline
            Pepsi-Mountain Dew & 1.3710           & 1.565                         & \multicolumn{1}{l|}{0.9623}                       & \multicolumn{1}{l|}{0.6028}                    \\ \hline
            Coke               & 0.9183           & 1.565                         &                                                   &                                                \\ \cline{1-3}
        \end{tabular}
    }
\end{table}

No, they should not be grouped; the respective information gains for the groupings are less than the information gain without grouping.

\section{J48}
For the deepest subtree, \texttt{credit\_history}, we get the following error for the individual leaves:

\begin{align*}
    U_{90\%}(0,\, 1) +
    U_{90\%}(0,\, 1) +
    7\times U_{90\%}(1,\, 7) +
    0 +
    2\times U_{90\%}(0,\, 2) &\approx\\
    0.95 +
    0.95 +
    7\times 0.5207 +
    0 +
    2\times 0.77639 &\approx 3.19709
\end{align*}

If the subtree would just be replaced by a leaf \textbf{good (9/2)}, predicted error would be

\begin{equation*}
    9\times U_{90\%}(2,\, 9) \approx 9\times 0.47009 \approx 4.23081
\end{equation*}

Therefore, the new tree would be

\begin{verbatim}
property_magnitude = car
|    personal_status = male div/sep: bad (1.0)
|    personal_status = female div/dep/mar: good (7.0)
|    personal_status = male single: good (9.0/2.0)
|    personal_status = male mar/wid: good (1.0)
|    personal_status = female single: good (0.0)
\end{verbatim}

For this subtree, we get the following error for individual leaves:

\begin{align*}
    U_{90\%}(0,\, 1) +
    7 \times U_{90\%}(0,\, 7) +
    9\times U_{90\%}(2,\, 9) +
    U_{90\%}(0,\, 1) +
    0 &\approx \\
    0.95 +
    7\times 0.34816
    9\times 0.47009
    0 &\approx 7.61793
\end{align*}

Again, if the subtree would be replaced by a leaf \textbf{good (17/2)}, predicted error would be

\begin{equation*}
    17\times U_{90\%}(2,\, 17) \approx 17\times 0.70420 \approx 5.0286
\end{equation*}

Therefore, the new tree would be

\begin{verbatim}
property_magnitude = car: good (17/2)
\end{verbatim}

\section{Missing Value}
\subsection{Outdoor}
\subsubsection{Hair = Short}
\begin{equation*}
    \text{entropy} =
    -\frac{1 + \nicefrac{2}{7}}{2 + \nicefrac{2}{7}} \log_2 {\frac{1 + \nicefrac{2}{7}}{2 + \nicefrac{2}{7}}}
    -\frac{1}{2 + \nicefrac{2}{7}} \log_2 {\frac{1}{2 + \nicefrac{2}{7}}}
    = 0.9887
\end{equation*}

\subsubsection{Hair = Long}
\begin{equation*}
    \text{entropy} =
    \frac{0}{\vdots} \cdots
    = 0
\end{equation*}


\subsection{Indoor}
\subsubsection{Hair = Short}
\begin{equation*}
    \text{entropy} =
    -\frac{1}{1 + \nicefrac{2}{7}} \log_2 {\frac{1}{1 + \nicefrac{2}{7}}}
    -\frac{\nicefrac{2}{7}}{1 + \nicefrac{2}{7}} \log_2 {\frac{\nicefrac{2}{7}}{2 + \nicefrac{2}{7}}}
    = 0.7642
\end{equation*}

\subsubsection{Hair = Long}
\begin{equation*}
    \text{entropy} =
    -\frac{1}{1} \log_2 {\frac{1}{1}}
    = 0
\end{equation*}

\subsection{Both}
\subsubsection{Hair = Short}
\begin{equation*}
    \text{entropy} =
    -\frac{1 + \nicefrac{3}{7}}{1 + \nicefrac{3}{7}} \log_2 {\frac{1 + \nicefrac{3}{7}}{2 + \nicefrac{3}{7}}} = 0
    = 0
\end{equation*}

\subsubsection{Hair = Long}
\begin{equation*}
    \text{entropy} =
    -\frac{1}{2} \log_2 {\frac{1}{2}}
    -\frac{1}{2} \log_2 {\frac{1}{2}}
    = 1
\end{equation*}

\end{document}
