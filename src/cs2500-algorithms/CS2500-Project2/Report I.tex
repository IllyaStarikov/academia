\documentclass{article}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}

\title{Programming Project II, First Report}
\author{Illya Starikov, Claire Trebing, Timothy Ott}
\date{Due Date: April 22, 2016}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\newcommand{\shellcmd}[1]{\texttt{\colorbox{gray!30}{#1}}}

\begin{document}
\maketitle

\section{Abstract}
Social Networks have revolutionized the way we communicate, meet others, consume information, and essentially influence our day-to-day lives. To show Social Network's prominence, \href{http://www.pewinternet.org/fact-sheets/social-networking-fact-sheet/}{here are the percentages of online adults who use social media}:

\begin{description}
    \item [Facebook]: 71\% Adults
    \item [Twitter]: 23\% Adults
    \item [Instagram]: 26\% Adults
    \item [Pinterest]: 28\% Adults
    \item [LinkedIn]: 28\% Adults
\end{description}

This is unprecedented. Seeing as an overwhelming majority of adults have some sort of social media account, this can be used to model real world relationships --- through social graphs.

In this experiment we would like to examine the social graph through the follower-followee relationship.

\section{Introduction and Motivation}
As stated above, social networks play a dominant role in our lives. Through social graph's, we can examine real world relationships.

There are many reasons for this to be valuable, such as common interest, community detection, influence amongst followers, etc. For this experiment, we would just like to test on the following measures:

\begin{description}
    \item [Degree Distribution] Test the direct amount of followers compared to followees a person has.
    \item [Shortest Path Distribution] Test the degree of separation between a follower-followee.
    \item [Graph Diameter] Test the breadth of a community.
    \item [Closeness Centrality] Test the dependence of a degree of separation on a singular person.
    \item [Betweenness Centrality Distribution] Same as previous.
    \item [Community Detection] Based on Closeness Centrality, find the diameter of a community.
\end{description}

This will give us a reasonable dataset for the relationship of a community in a social graph.

\section{Proposed Solutions}
\subsection{Uweighted In Degree Distribution}
Unweighted In Degree Distribution goes through each entry to determine what the origin of each edge is. An array with cells for each possible vertex (total) is created and once the origin is determined the respective cell in total is increased by one. After all of the entries have been checked, the function goes through the totals.

The functions begins by assuming \texttt{total[0]} is the maximum number of In Degree edges and $0$ is added to \texttt{arrayMax}. \texttt{arrayMax} is the array which holds all of the vertexes with the maximum number of in degree edges. Each entry in total is compared to max. If the total entry is larger than the current max, then the current max is replaced with current total. The \texttt{arrayMax} list is cleared and the current total origin is added to the \texttt{arrayMax}. If the current total is equal to the max, then the origin of current total is added to \texttt{arrayMax}.

\begin{verbatim}
void Unweighted_InDegree_Distribution(&int arrayMax[])
{
    N = the number of vertices
    int * total;
    total = new int [maxVertice];

    for(i = 0 to N){
        total[origin of i]++;
    }
    int degreeMax = total[0];
    add 0 to arrayMax

    for(i = 1 to maxVertice){
        if(total[i] > degreeMax){
            degreeMax = total[i];
            clear arrayMax
            add i to arrayMax
        }
        if(total[i] == degreeMax){
            add i to arrayMax
        }
    }
    delete[] total;
    return;
}
\end{verbatim}

The time complexity of this function is $2n = \mathcal{O}(n)$. The function must traverse through the array twice before finishing.

\subsection{Unweighted Out Degree Distribution}
Unweighted Out Degree Distribution goes through each entry to determine what the destination of each edge is. An array with cells for each possible vertex (total) is created
and once the destination is determined the respective cell in total is increased by one. After all of the entries have been checked, the function goes through the totals.
The functions begins by assuming \texttt{total[0]} is the maximum number of Out Degree edges and $0$ is added to \texttt{arrayMax}. \texttt{arrayMax} is the array which holds all of the vertexes with the maximum number of out degree edges. Each entry in total is compared to max. If the total entry is larger than the current max, then the current max is replaced with current
total. The \texttt{arrayMax} list is cleared and the current total destination is added to the \texttt{arrayMax}. If the current total is equal to the max, then the destination of current total is added to the \texttt{arrayMax}.

\begin{verbatim}
void Unweighted_OutDegree_Distribution(&int arrayMax[])
{
    N = the number of vertices
    int * total;
    total = new int [maxDestination];

    for(i = 0 to N){
        total[Destination of i]++;
    }
    int degreeMax = total[0];
    add 0 to arrayMax

    for(i = 1 to maxOrigin){
        if(total[i] > degreeMax){
            degreeMax = total[i];
            clear arrayMax
            add i to arrayMax
        }
        if(total[i] == degreeMax){
            add i to arrayMax
        }
    }
    delete[] total;
    return;
}
\end{verbatim}

\subsubsection{Complexity Analysis}
The time complexity of this function is $2n = \mathcal{O}(n)$. The function must traverse through the array twice before finishing.

\subsection{Weighted In Degree Distribution}
Weighted In Degree Distribution goes through each entry to determine what the origin of each edge is and the sum of the weighted edges. An array with cells for each possible vertex (total) is created and once the origin is determined the respective cell in total is increased by weight of that edge. After all of the entries have been checked, the function goes through the totals.

The functions begins by assuming \texttt{total[0]} is the maximum number of Weighted In Degree edges and $0$ is added to \texttt{arrayMax}. \texttt{arrayMax} is the array which holds all of the vertexes with the maximum number of weighted in degree edges. Each entry in total is compared to max. If the total entry is larger than the current max, then the current max is replaced with current total. The \texttt{arrayMax} list is cleared and the current total origin is added to the arrayMax. If the current total is equal to the max, then the origin of current total is added to the \texttt{arrayMax}.

\begin{verbatim}
void Weighted_InDegree_Diribution(&int arrayMax[])
{
    N = the number of vertices
    int * total;
    total = new int [maxOrigin];

    for(i = 0 to N){
        total[Origin of i]+= (Weight of i);
    }
    int degreeMax = total[0];
    add 0 to arrayMax

    for(i = 1 to maxOrigin){
        if(total[i] > degreeMax){
            degreeMax = total[i];
            clear arrayMax
            add i to arrayMax
        }
        if(total[i] == degreeMax){
            add i to arrayMax
        }
    }
    delete[] total;
    return;
}
\end{verbatim}

\subsubsection{Complexity Analysis}
The time complexity of this function is $2n = \mathcal{O}(n)$. The function must traverse through the array twice before finishing.

\subsection{Weighted Out Degree Distribution}
Weighted Out Degree Distribution goes through each entry to determine what the destination of each edge is and the weight of those edges. An array with cells for each possible vertex (total) is created and once the destination is determined the respective cell in total is increased by the value of that edge. After all of the entries have been checked, the function goes through the totals. The functions begins by assuming \texttt{total[0]} is the maximum number of Weighted Out Degree edges and $0$ is added to \texttt{arrayMax}. \texttt{arrayMax} is the array which holds all of the vertexes with the maximum number of weighted out degree edges. Each entry in total is compared to max. If the total entry is larger than the current max, then the current max is replaced with current total. The \texttt{arrayMax} list is cleared and the current total destination is added to the arrayMax. If the current total is equal to the max, then the destination of current total is added to the \texttt{arrayMax}.

\begin{verbatim}
void Weighted_OutDegree_Diribution(&int arrayMax[])
{
    N = the number of vertices
    int * total;
    total = new int [maxDestionation];

    for(i = 0 to N){
        total[Destination of i]+= (Weight of i);
    }
    int degreeMax = total[0];
    add 0 to arrayMax

    for(i = 1 to maxDestination){
        if(total[i] > degreeMax){
            degreeMax = total[i];
            clear arrayMax
            add i to arrayMax
        }
        if(total[i] == degreeMax){
            add i to arrayMax
        }
    }
    delete[] total;
    return;
}
\end{verbatim}

The time complexity of this function is $2n = O(n)$. The function must traverse through the array twice before finishing.

\subsection{Shortest Path}
\begin{verbatim}
V = the number of vertices
distance = new array[V][V]
Initialize distance to -1   //since there are no negative weights this will
                            //represent infinity
Floyd(V):
    for each vertex v:
        distance[v][v] = 0
    for each edge (u,v)
        distance[u][v] = w(u,v) //the weight of the edge (u,v)
    for k from 1 to V
        for i from 1 to V
            for j from 1 to V
              if distance[i][j] > distance[i][k] + distance[k][j]
                distance[i][j] = distance[i][k] + distance[k][j]

shortest = new array[100]
initialize shortest to 0
for i from 1 to V
    for j from 1 to V
        if distance[i][j] > 0
          if distance[i][j] > shortest.length
              temp = new array[distance[i][j]+10]
              for i from 0 to shortest.length
                  temp[i] = distance[i]
              delete distance
              distance = temp
          shortest[distance[i][j]]++

for i from 1 to shortest.length
    if shortest[i] > 0
        output shortest[i]
\end{verbatim}

\subsubsection{Complexity Analysis}
As we know, the Floyd-Warshall algorithm operates on $n^3$ time. Combining this with the algorithm to iterate over the resulting V x V matrix results in a $n^2 + n^3$ complexity --- or, $\mathcal{O}(n^3)$ time.

\subsection{Unweighted Graph Diameter}
Our solution first creats an $N \times N$ array to hold all of the shortest paths from each vertex to every other
vertex. For an unweighted graph the shortest path from $i \rightarrow j$ is equal to the shortest path from $j \rightarrow i$. After creating and filling the matrix, the function assumes that the value at \texttt{distance[0][1]}. The function then tests every value in the matrix and compares it to the max. If the value is greater than the max then the max is replaced
with \texttt{distance[i][j]}. Clear the \texttt{GraphDiameterList} and then add $(i,j)$ to the graph diameter list. If \texttt{distance[i][j]} is equal to max then add $(i,j)$ to the \texttt{GraphDiameterList}.

The pseudocode is as follows.

\begin{verbatim}
Unweighted Graph Diameter {
    N = the number of vertices
    distance = new array[N][N]
    all distances start at -1

    for(i = 0 to N){
        for(j = i+1 to N){
            distance[i][j] = shortest path(i, j)
        }
    }

    max = Diameter[0][1]
    Add (0,1) to GraphDiameterList

    for(i = 0 to N){
        for(j = 1 to N){
            if(distance[i][j] > max){
                clear GraphDiameterList
                max = distance[i][j]
                add (i,j) to GraphDiameterList
            }
            if(distance[i][j] = max){
                add (i,j) to GraphDiameterList
            }
    }
    return GraphDiameterList
}
\end{verbatim}

\subsubsection{Complexity Analysis}
The time complexity of this code is $\mathcal{O}(n^2)$. You must go through the matrix twice to complete this function.

\subsection{Weighted Graph Diameter}
This functions first created an $N \times N$ array to hold all of the shortest paths from each vertex to every other
vertex. After creating and filling the matrix, the solution assumes that the value at \texttt{distance[0][1]}. The function then tests every value in the matrix and compares it to the max. If the value is greater than the max then the max is replaced
with \texttt{distance[i][j]}. Clear the \texttt{GraphDiameterList} and then add $(i, j)$ to the graph diameter list. If \texttt{distance[i][j]} is equal to max then add $(i, j)$ to the \texttt{GraphDiameterList}.

The pseudocode is as follows:

\begin{verbatim}
Weighted Graph Diameter{
    N = the number of vertices
    distance = new array[N][N]
    all distances start at -1

    for(i = 0 to N){
        for(j = 0 to N){
            if(i != j){
                distance[i][j] = shortest path(i, j)
            }
        }
    }

    max = Diameter[0][1]
    Add (0,1) to GraphDiameterList

    for(i = 0 to N){
        for(j = 0 to N){
            if(i != j)
            if(distance[i][j] > max){
                clear GraphDiameterList
                max = distance[i][j]
                add (i,j) to GraphDiameterList
            }
            if(distance[i][j] = max){
                add (i,j) to GraphDiameterList
            }
    }
    return GraphDiameterList
\end{verbatim}

\subsubsection{Complexity Analysis}
The time complexity of this code is $\mathcal{O}(n^2)$. You must go through the matrix twice to complete this function.

\subsection{Closeness Centrality}
\begin{verbatim}
sum = 0
closeness = new array[100]
initialize closeness to 0
for i from 0 to V
    for j from 1 to V
        sum += distance[i][j]
    closeness[i] = 1/sum
    output closeness[i]
    sum = 0
\end{verbatim}

\subsubsection{Complexity Analysis}
Because we are iterating over the entire matrix resulting from the Floyd-Warshall algorithm of $V \times V$ size. We arrive at a complexity of $\mathcal{O}(n^2)$.

\subsection{Undirected Betweenness Centrality Distribution}
This algorithms takes a vertex $i$ from the main function. The function then looks at every shortest path possible. Starting at vertex $0$ the function and going to the largest vertex. If $i$ is the starting point $(j)$ or the ending point $(k)$ it is ignored. $K$ is defined as $j + 1$ If the shortest path because in undirected graphs, the shortest distance from $j \rightarrow k$ == the shortest distance from $k \rightarrow j$. If the shortest distance from j to k includes the vertex i, the value of the betweenness is increased by 1. The total betweens is then returned to the main function.

\begin{verbatim}
Undirected Betweenness Centrality Distribution of Vetex i(vertex i){
    N = max vertex
    B = betweenses = 0
    for(j = 0 to N){
        if(i != j){
            for(k = j+1 to N){
                if(i != k){
                    shortestpath between j and k
                    if i is included, increase betweeneses by 1
                }
            }
        }
    }

    return B;
}
\end{verbatim}

This function has a complexity of $n$. Each vertex check every vertex larger than itself to determine all of the shortest paths. The first vertex must check nearly other ever vertex, except $i$. This gives it a complexity of $\mathcal{O}(n)$.

\subsection{Directed Betweenness Centrality Distribution}
This algorithm takes a vertex $i$ from the main function. The function then looks at every shortest path possible. Starting at vertex $0$ the function and going to the largest vertex. If i is the starting point $(j)$ or the ending point $(k)$ it is ignored. The path is also ignored if j and k are equal. If the shortest distance from j to k includes i, then the betweenness is increased by $1$. The total betweenness is then returned to the main function.

\begin{verbatim}
Directed Betweenness Centrality Distribution of Vertex i(vertex i){
    N = max vertex
    B = betweenses = 0
    for(j = 0 to N){
        if(i != j){
            for(k = 0 to N){
                if(i != k && j != k){
                    shortest path between j and k
                    if i is included, increase betweenses by 1
                }
            }
        }
    }
}
\end{verbatim}

\subsubsection{Complexity Analysis}
This function has a complexity of $n^2$. Each vertex must check every other vertex to determine
all of the shortest paths. This gives it a complexity of $\mathcal{O}(n^2)$.

\subsection{Undirected Unweighted Betweenness Centrality Distribution}
This algorithm takes an edge e from the main function. The function then looks at every shortest path possible. Starting at vertex 0 the function and going to the largest vertex. K is defined as j + 1 because the shortest path in undirected graphs, the shortest distance from j -> k == the shortest distance from k -> j. If the shortest distance from j to k includes the edge e, the value of the betweenness is increased by 1. The total betweenness is then returned to the main function.

\begin{verbatim}
Undirected Unweighted Betweenness Centrality Distribution of Edge e(edge e){
    N = max vertex
    B = betweenses = 0
    for(j = 0 to N){
        for(k = j+1 to N){
            shortest path between j and k
            if e is included, increase betweeneses by 1
            }
        }
    }
    return B;
}
\end{verbatim}

\subsubsection{Complexity Analysis}
This function has a complexity of $n$. Each vertex check every vertex larger than itself to determine all of the shortest paths. The first vertex must check nearly other ever vertex. This gives it a complexity of $n$.

\subsection{Directed Unweighted Betweenness Centrality Distribution of Edge}
This algorithm takes an edge $e$ from the main function. The function then looks at every shortest path possible. Starting at vertex $0$ the function and going to the largest vertex. If the shortest distance from $j$ to $k$ includes the edge $e$, the value of the betweenness is increased by 1. The total betweenness is then returned to the main function.

\begin{verbatim}
Directed Unweighted Betweenness Centrality Distribution of Edge e(edge e){
    N = max vertex
    B = betweenses = 0
    for(j = 0 to N){
        for(k = 0 to N){
            if(j != k){
                shortest path between j and k
                if e is included, increase betweeneses by 1
            }
        }
    }
    return B;
}
\end{verbatim}

\subsubsection{Complexity Analysis}
This function has a complexity of $n^2$. Each vertex must check every other vertex to determine
all of the shortest paths. This gives it a complexity of $\mathcal{O}(n^2)$.

\subsection{Undirected Weighted Betweenness Centrality Distribution of Edge e}
This algorithm takes an edge e from the main function. The function then looks at every shortest path possible. Starting at vertex $0$ the function and going to the largest vertex. $K$ is defined as j + 1 because the shortest path in undirected graphs, the shortest distance from $j \rightarrow k$ == the shortest distance from $k \rightarrow j$. If the shortest distance from $j$ to $k$ includes the edge $e$, the value of the betweenness is increased by $1$. The total betweenness is then returned to the main function divided by the weight of e.

\begin{verbatim}
Undirected Weighted Betweenness Centrality Distribution of Edge e(edge e){
    N = max vertex
    B = betweenness = 0
    for(j = 0 to N){
        for(k = j+1 to N){
            shortest path between j and k
            if e is included, increase betweenness by 1
            }
        }
    }
    return B/(weight of e);
}
\end{verbatim}

\subsubsection{Complexity Analysis}
This function has a complexity of $n$. Each vertex check every vertex larger than itself to determine all of the shortest paths. The first vertex must check nearly other ever vertex. This gives it a complexity of $\mathcal{O}(n)$.

\begin{verbatim}
Directed Weighted Betweenness Centrality Distribution of Edge e(edge e){
    N = max vertex
    B = betweenness = 0
    for(j = 0 to N){
        for(k = 0 to N){
            if(j != k){
                shortest path between j and k
                if e is included, increase betweeneses by 1
            }
        }
    }
    return B/(weight of edge e);
}
\end{verbatim}

This algorithms takes an edge $e$ from the main function. The function then looks at every shortest path possible. Starting at vertex $0$ the function and going to the largest vertex. If the shortest distance from j to k includes the edge e, the value of the betweenness is increased by $1$. The total betweenness is then returned to the main function divided by the edge $e$.

\subsubsection{Complexity Analysis}
This function has a complexity of $n^2$. Each vertex must check every other vertex to determine all of the shortest paths. This gives it a complexity of $\mathcal{O}(n^2)$.

\subsection{Community Detection}
\begin{verbatim}
UWBetween[] = Unweighted Betweenness Centrality Edges
V = Original Network

For k from 0 to 4:
    DescendSort(UWBetween)
    UWBetween[0] = x
  while (UWBetween[0] == x)
      V.remove(UWBetween[i])
      UWBetween.remove(UWBetween[0])
  Floyd(V)            //Run Floyd-Warshall Algorithm on revised data set
  Diameter(V)         // Calculate and output max diameter based on new matrix from above
  Betweenness(V)      // Calculate Betweenness centrality based on
                      // new matrix giving us a new UWBetween array
\end{verbatim}

\subsubsection{Complexity Analysis}
Because the Community detection algorithm calls the algorithms to find all shortest paths, the diameter and the unweighted betweenness centrality edges within it and since we are executing this algorithm a total of five times, the complexity of this algorithm is five times the sum of the complexities of these algorithms.

\section{Plan of Experiments}
The major purpose of our experiment is to dissect and examine networks, so we propose the following plan of experiments:

\begin{enumerate}
    \item Extrapolate data and store in a relevant data structure --- in our case, a sorted \texttt{map<int, vector<pair<int, double>>>}
    \begin{itemize}
        \item The \texttt{int} is the key to be used, signifying the origin.
        \item The \texttt{vector<pair>} holds all the adjacent edges.
    \end{itemize}
    \item Iterate over the entirety of the data structure to determine degree distribution.
    \begin{itemize}
        \item For \emph{weighted and unweighted out degree}, it is simply counting the the \texttt{vector} size with respect to each key.
        \item For \emph{weighted and unweighted in degree}, making an efficient algorithm is still difficult --- not only is by default $\mathcal{O}(n)$ but there is an efficient way of finding where the edges lead to. We accomplish this by a \href{https://en.wikipedia.org/wiki/Binary_search_tree}{Binary Search Tree}. By iterating over every vertex and storing their destination in a binary search tree, we have achieved a sufficient algorithm.
    \end{itemize}
    \item Detect the shortest path via the \href{https://en.wikipedia.org/wiki/Floydâ€“Warshall_algorithm}{Floyd-Warshall algorithm} for directed graphs.
        \begin{itemize}
            \item Make the graph undirected by making it symmetric about the $a _{i, i}$ elements $\forall i \in \text{edges}$.
            \item Test again.
        \end{itemize}
    \item Detect closeness centrality and betweenness.
    \item Implement community detection.
    \item Output results to user.
\end{enumerate}

\section{Team Roles}
\begin{itemize}
    \item Illya Starikov
    \begin{itemize}
        \item Project Manager
        \item Official Write-up
    \end{itemize}
    \item Timothy Ott
    \begin{itemize}
        \item Pseudocode Write-up
        \item Algorithm Analysis
    \end{itemize}
    \item Claire Trebing
    \begin{itemize}
        \item Pseudocode Write-up
        \item Algorithm Analysis
    \end{itemize}
\end{itemize}

\end{document}